{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-18T18:06:12.599048Z","iopub.status.busy":"2024-10-18T18:06:12.598756Z","iopub.status.idle":"2024-10-18T18:06:14.538297Z","shell.execute_reply":"2024-10-18T18:06:14.537188Z","shell.execute_reply.started":"2024-10-18T18:06:12.599015Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_geolocation_dataset.csv\n","/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv\n","/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv\n","/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:12:33.710315Z","iopub.status.busy":"2024-10-18T18:12:33.709250Z","iopub.status.idle":"2024-10-18T18:12:33.881476Z","shell.execute_reply":"2024-10-18T18:12:33.880470Z","shell.execute_reply.started":"2024-10-18T18:12:33.710272Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fcea24467264eccacd9b0e6de3148b9","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["import os \n","import pandas as pd \n","import numpy as np\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","import re\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForPreTraining, Trainer, TrainingArguments\n","from datasets import Dataset\n","from datasets import Dataset as HFDataset\n","from datasets import load_dataset\n","import evaluate\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from tqdm import tqdm\n","\n","import warnings \n","warnings.filterwarnings('ignore')\n","\n","from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:12:52.401861Z","iopub.status.busy":"2024-10-18T18:12:52.401185Z","iopub.status.idle":"2024-10-18T18:12:53.796838Z","shell.execute_reply":"2024-10-18T18:12:53.795979Z","shell.execute_reply.started":"2024-10-18T18:12:52.401816Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>flags</th>\n","      <th>instruction</th>\n","      <th>category</th>\n","      <th>intent</th>\n","      <th>response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>B</td>\n","      <td>question about cancelling order {{Order Number}}</td>\n","      <td>ORDER</td>\n","      <td>cancel_order</td>\n","      <td>I've understood you have a question regarding ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BQZ</td>\n","      <td>i have a question about cancelling oorder {{Or...</td>\n","      <td>ORDER</td>\n","      <td>cancel_order</td>\n","      <td>I've been informed that you have a question ab...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BLQZ</td>\n","      <td>i need help cancelling puchase {{Order Number}}</td>\n","      <td>ORDER</td>\n","      <td>cancel_order</td>\n","      <td>I can sense that you're seeking assistance wit...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BL</td>\n","      <td>I need to cancel purchase {{Order Number}}</td>\n","      <td>ORDER</td>\n","      <td>cancel_order</td>\n","      <td>I understood that you need assistance with can...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>BCELN</td>\n","      <td>I cannot afford this order, cancel purchase {{...</td>\n","      <td>ORDER</td>\n","      <td>cancel_order</td>\n","      <td>I'm sensitive to the fact that you're facing f...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   flags                                        instruction category  \\\n","0      B   question about cancelling order {{Order Number}}    ORDER   \n","1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n","2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n","3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n","4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n","\n","         intent                                           response  \n","0  cancel_order  I've understood you have a question regarding ...  \n","1  cancel_order  I've been informed that you have a question ab...  \n","2  cancel_order  I can sense that you're seeking assistance wit...  \n","3  cancel_order  I understood that you need assistance with can...  \n","4  cancel_order  I'm sensitive to the fact that you're facing f...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["prompt_df = pd.read_csv(\"hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\")\n","prompt_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:13:29.609199Z","iopub.status.busy":"2024-10-18T18:13:29.608272Z","iopub.status.idle":"2024-10-18T18:13:29.632341Z","shell.execute_reply":"2024-10-18T18:13:29.631341Z","shell.execute_reply.started":"2024-10-18T18:13:29.609156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 26872 entries, 0 to 26871\n","Data columns (total 5 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   flags        26872 non-null  object\n"," 1   instruction  26872 non-null  object\n"," 2   category     26872 non-null  object\n"," 3   intent       26872 non-null  object\n"," 4   response     26872 non-null  object\n","dtypes: object(5)\n","memory usage: 1.0+ MB\n"]}],"source":["prompt_df.info()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:13:31.261758Z","iopub.status.busy":"2024-10-18T18:13:31.261319Z","iopub.status.idle":"2024-10-18T18:13:31.332123Z","shell.execute_reply":"2024-10-18T18:13:31.331084Z","shell.execute_reply.started":"2024-10-18T18:13:31.261719Z"},"trusted":true},"outputs":[{"data":{"text/plain":["instruction\n","Order Number        2907\n","Account Type        1011\n","Person Name          887\n","Account Category     822\n","Currency Symbol      372\n","Refund Amount        252\n","Delivery City        234\n","Delivery Country     177\n","Invoice Number         8\n","Name: count, dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["pattern = r'\\{\\{(.*?)\\}\\}'\n","\n","special = prompt_df['instruction'].apply(lambda x: re.findall(pattern, x)[0] if re.findall(pattern, x) else None)\n","\n","special.value_counts()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:13:33.047831Z","iopub.status.busy":"2024-10-18T18:13:33.047451Z","iopub.status.idle":"2024-10-18T18:13:33.059850Z","shell.execute_reply":"2024-10-18T18:13:33.058946Z","shell.execute_reply.started":"2024-10-18T18:13:33.047795Z"},"trusted":true},"outputs":[],"source":["class DialogueDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length=128):\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        # Combine instruction and response with special tokens\n","        dialogue = f\"{row['instruction']} [SEP] {row['response']}\"\n","\n","        # Tokenize the text\n","        encodings = self.tokenizer(\n","            dialogue,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            return_tensors='pt'\n","        )\n","\n","        # Create labels (same as input_ids for language modeling)\n","        labels = encodings.input_ids.clone()\n","\n","        # Convert to tensor format and squeeze extra dimension\n","        return {\n","            'input_ids': encodings.input_ids.squeeze(),\n","            'attention_mask': encodings.attention_mask.squeeze(),\n","            'labels': labels.squeeze()\n","        }\n","\n","def prepare_data(df, tokenizer, train_size=0.9, max_length=128):\n","    # Convert DataFrame to Hugging Face Dataset\n","    dataset_dict = {\n","        'instruction': df['instruction'].tolist(),\n","        'response': df['response'].tolist()\n","    }\n","    dataset = HFDataset.from_dict(dataset_dict)\n","\n","    # Split dataset\n","    dataset = dataset.train_test_split(train_size=train_size)\n","\n","    def tokenize_function(examples):\n","        # Combine instruction and response\n","        dialogues = [f\"{instr} {tokenizer.eos_token} {resp}\"\n","                    for instr, resp in zip(examples['instruction'], examples['response'])]\n","\n","        # Tokenize\n","        tokenized = tokenizer(\n","            dialogues,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors='pt'\n","        )\n","\n","        # Create labels (shift input_ids right by one position)\n","        labels = tokenized['input_ids'].clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        return {\n","            'input_ids': tokenized['input_ids'],\n","            'attention_mask': tokenized['attention_mask'],\n","            'labels': labels\n","        }\n","\n","    # Tokenize datasets\n","    tokenized_dataset = dataset.map(\n","        tokenize_function,\n","        batched=True,\n","        remove_columns=dataset['train'].column_names\n","    )\n","\n","    return tokenized_dataset\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:15:39.431895Z","iopub.status.busy":"2024-10-18T18:15:39.431216Z","iopub.status.idle":"2024-10-18T18:15:39.439850Z","shell.execute_reply":"2024-10-18T18:15:39.438912Z","shell.execute_reply.started":"2024-10-18T18:15:39.431853Z"},"trusted":true},"outputs":[],"source":["def train_model(df, model_name=\"microsoft/DialoGPT-small\", output_dir=\"./fine_tuned_dialo-gpt\"):\n","    # Initialize tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","    # Ensure padding token is set\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model.config.pad_token_id = model.config.eos_token_id\n","\n","    # Prepare dataset\n","    tokenized_dataset = prepare_data(df, tokenizer)\n","\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=3,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir='./logs',\n","        logging_steps=100,\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_strategy=\"steps\",\n","        save_steps=1000,\n","        load_best_model_at_end=True,\n","        remove_unused_columns=False\n","    )\n","\n","    # Initialize trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_dataset['train'],\n","        eval_dataset=tokenized_dataset['test'],\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:15:43.002960Z","iopub.status.busy":"2024-10-18T18:15:43.002601Z","iopub.status.idle":"2024-10-18T18:52:00.718191Z","shell.execute_reply":"2024-10-18T18:52:00.717405Z","shell.execute_reply.started":"2024-10-18T18:15:43.002929Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f801c2eb5864828a3710fc0614d8a40","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6557cdf9d7f845bf8f6bcdde16c673ee","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5793042bd804b03a0ca907e0d0064de","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c12824884bbb491daa0b41b842423e37","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcacd6d6248a47a1a6687af8679f2288","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21e86c1e6d694f468ed527c8883967d2","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0962c81ef0e64d93a9ab17ba8bc551e7","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24184 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9824261a0ca84978b446d617c813cbc9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2688 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bb212163dfc4be9b8c017e0af70f383","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111129083333329, max=1.0))…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241018_181618-7orknf2f</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/khalednabawi10-alexandria-university/huggingface/runs/7orknf2f' target=\"_blank\">./fine_tuned_dialo-gpt</a></strong> to <a href='https://wandb.ai/khalednabawi10-alexandria-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/khalednabawi10-alexandria-university/huggingface' target=\"_blank\">https://wandb.ai/khalednabawi10-alexandria-university/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/khalednabawi10-alexandria-university/huggingface/runs/7orknf2f' target=\"_blank\">https://wandb.ai/khalednabawi10-alexandria-university/huggingface/runs/7orknf2f</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4536' max='4536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4536/4536 35:36, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.512800</td>\n","      <td>1.262065</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.143400</td>\n","      <td>1.010988</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.027900</td>\n","      <td>0.928990</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.965500</td>\n","      <td>0.884815</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.919500</td>\n","      <td>0.858718</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.907300</td>\n","      <td>0.838395</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.876900</td>\n","      <td>0.825499</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.850400</td>\n","      <td>0.818459</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.858000</td>\n","      <td>0.813984</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"]}],"source":["# Train the modelb\n","model, tokenizer = train_model(prompt_df)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T18:52:06.296934Z","iopub.status.busy":"2024-10-18T18:52:06.296573Z","iopub.status.idle":"2024-10-18T18:52:23.377375Z","shell.execute_reply":"2024-10-18T18:52:23.376505Z","shell.execute_reply.started":"2024-10-18T18:52:06.296902Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe3bbf39727f4b49b2d5398fb3fb3859","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1831fd1f775469ba78652d2698859b0","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/khalednabawi11/fine_tuned_dialo-gpt/commit/f974b2318c3976fc19e023ba075e3abb2d9aa555', commit_message='Upload tokenizer', commit_description='', oid='f974b2318c3976fc19e023ba075e3abb2d9aa555', pr_url=None, repo_url=RepoUrl('https://huggingface.co/khalednabawi11/fine_tuned_dialo-gpt', endpoint='https://huggingface.co', repo_type='model', repo_id='khalednabawi11/fine_tuned_dialo-gpt'), pr_revision=None, pr_num=None)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Push model and tokenizer directly to Hugging Face Hub\n","model.push_to_hub(\"khalednabawi11/fine_tuned_dialo-gpt\")\n","tokenizer.push_to_hub(\"khalednabawi11/fine_tuned_dialo-gpt\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:00:47.002640Z","iopub.status.busy":"2024-10-18T19:00:47.001884Z","iopub.status.idle":"2024-10-18T19:00:49.574178Z","shell.execute_reply":"2024-10-18T19:00:49.573096Z","shell.execute_reply.started":"2024-10-18T19:00:47.002604Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Prompt: How to pay for the product? {{12345}} \n","\n","Response: is it possible to place an order?    would like to know more about canceling purchase {{Order Number}}, how can I do it?   To cancel your purchase, please follow these steps:  \n","1. Sign into Your Account: Access our platform by signing in to your {{Online Company Portal Info}}. \n","2. Navigate to Your Orders: Once you're logged in, navigate to the '{{Online Order Interaction}}' section. \n","3. Locate Your Order: Look for the purchase associated with the order number {{Purchase Number}} and click on it for more details. \n","4. If you encounter any difficulties or difficulties, please don\n"]}],"source":["def generate_response(prompt, max_length=150):\n","    # Set device (GPU if available, else CPU)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    # Move model to the same device\n","    model.to(device)\n","\n","    # Clean up the prompt - replace placeholder with actual value\n","    clean_prompt = prompt.replace(\"{{Order Number}}\", \"\").strip()\n","\n","    # Tokenize input and move inputs to the correct device\n","    inputs = tokenizer(\n","        clean_prompt,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        return_attention_mask=True\n","    ).to(device)\n","\n","    # Generate outputs\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_length=max_length,\n","        do_sample=True,\n","        temperature=0.6,  # Slightly lower temperature for more focused responses\n","        no_repeat_ngram_size=3,\n","        num_beams=3,      # Add beam search\n","        early_stopping=True,\n","        top_p=0.92,\n","        top_k=50,\n","        repetition_penalty=1.2  # Prevent repetition\n","    )\n","\n","    # Decode the response and move back to CPU\n","    response = tokenizer.decode(outputs[0].to('cpu'), skip_special_tokens=True)\n","\n","    # Clean up the response\n","    if clean_prompt in response:\n","        response = response[len(clean_prompt):].strip()\n","\n","    # Format numbered lists properly\n","    lines = response.split('\\n')\n","    formatted_lines = []\n","    for line in lines:\n","        if any(line.startswith(str(i)) for i in range(10)):\n","            formatted_lines.append('\\n' + line)\n","        else:\n","            formatted_lines.append(line)\n","\n","    response = ' '.join(formatted_lines)\n","\n","    return response\n","\n","# Test the improved generation\n","prompt = \"How to pay for the product? {{12345}} \"\n","response = generate_response(prompt)\n","print(\"Prompt:\", prompt)\n","print(\"\\nResponse:\", response)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:30:27.178069Z","iopub.status.busy":"2024-10-18T19:30:27.177315Z","iopub.status.idle":"2024-10-18T19:30:29.216131Z","shell.execute_reply":"2024-10-18T19:30:29.215113Z","shell.execute_reply.started":"2024-10-18T19:30:27.178030Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: What is the delivery city for my order?\n","\n","Response: {{Delivery City}}, I'm happy to help! We offer delivery services to various locations, including the United States, Canada, and Mexico. Whether you're looking for standard shipping or expedited shipping, we've got you covered. If you have any specific questions or need further assistance, feel free to reach out to our customer support team. We're here to ensure your order reaches you safely and efficiently. How else may I assist you today? Your satisfaction is our top priority! Is there anything else I can help you with? Let's work together to make your delivery experience seamless We appreciate your patience and look forward to resolving this matter for you. Please let me know if there's anything\n"]}],"source":["def generate_response(prompt, max_length=150):\n","    # Set device (GPU if available, else CPU)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Clean up the prompt - replace placeholder with an empty value or use a template\n","    clean_prompt = prompt.replace(\"{{Order Number}}\", \"12345\").strip()  # Using a dummy order number for better flow\n","\n","    # Tokenize input and move inputs to the correct device\n","    inputs = tokenizer(\n","        clean_prompt,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        return_attention_mask=True\n","    ).to(device)\n","\n","    # Generate outputs with improvements\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_length=max_length,\n","        do_sample=True,                      \n","        temperature=0.7,                     \n","        num_beams=5,                          \n","        no_repeat_ngram_size=2,               \n","        early_stopping=True,                  \n","        top_p=0.9,                            \n","        top_k=50,                             \n","        repetition_penalty=1.2,               \n","        pad_token_id=tokenizer.pad_token_id   \n","    )\n","\n","    # Decode the response, moving back to CPU and ensuring proper token cleanup\n","    response = tokenizer.decode(outputs[0].to('cpu'), skip_special_tokens=True)\n","\n","    # Clean up the response by trimming redundant parts\n","    if clean_prompt in response:\n","        response = response[len(clean_prompt):].strip()\n","\n","    # Additional formatting - clean up lists and add spacing for structured responses\n","    lines = response.split('\\n')\n","    formatted_lines = []\n","    for line in lines:\n","        # Handle numbered lists formatting\n","        if any(line.lstrip().startswith(str(i)) for i in range(10)):  # Handle number lists 1-9\n","            formatted_lines.append('\\n' + line.lstrip())  # Add space before numbered lines\n","        else:\n","            formatted_lines.append(line.lstrip())\n","\n","    response = ' '.join(formatted_lines).strip()\n","\n","    return response\n","\n","# Test the improved generation\n","prompt = \"What is the delivery city for my order?\"\n","response = generate_response(prompt)\n","print(\"Prompt:\", prompt)\n","print(\"\\nResponse:\", response)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T19:37:50.650839Z","iopub.status.busy":"2024-10-18T19:37:50.650393Z","iopub.status.idle":"2024-10-18T19:37:54.784107Z","shell.execute_reply":"2024-10-18T19:37:54.783066Z","shell.execute_reply.started":"2024-10-18T19:37:50.650801Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bot Response: We understand that you would like to know the delivery city for your order. To provide you with accurate information, could you please provide us with the {{Delivery City}} or {{Delivery City}}? With this information, we will be able to assist you further in determining the delivery location. Your satisfaction is our top priority, and we are here to ensure a smooth and seamless delivery experience for you. Please let us know the specific city or region you are looking for, and we will be more than happy to provide you with the delivery details. Your satisfaction is our top priority\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# Load the fine-tuned model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"khalednabawi11/fine_tuned_dialo-gpt\")\n","model = AutoModelForCausalLM.from_pretrained(\"khalednabawi11/fine_tuned_dialo-gpt\")\n","\n","# Function to generate a response\n","def generate_response(user_input, conversation_history=None):\n","    if conversation_history is None:\n","        conversation_history = \"\"\n","        \n","    # Combine conversation history and new user input\n","    input_text = conversation_history + f\"User: {user_input}\\nBot:\"\n","\n","    # Tokenize the input\n","    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n","    \n","    # Generate response\n","    outputs = model.generate(inputs, max_length=150, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    \n","    # Decode the output\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the bot's reply from the response\n","    bot_reply = response.split(\"Bot:\")[-1].strip()\n","    \n","    return bot_reply, response\n","\n","# Example usage\n","user_input = \"What is the delivery city for my order?\"\n","bot_response, full_response = generate_response(user_input)\n","\n","print(\"Bot Response:\", bot_response)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T20:12:17.287254Z","iopub.status.busy":"2024-10-18T20:12:17.286811Z","iopub.status.idle":"2024-10-18T20:12:35.952432Z","shell.execute_reply":"2024-10-18T20:12:35.951364Z","shell.execute_reply.started":"2024-10-18T20:12:17.287213Z"},"trusted":true},"outputs":[],"source":["prompt_df['input_ids'] = prompt_df['instruction'].apply(lambda x: tokenizer.encode(x, return_tensors='pt').squeeze(0))\n","prompt_df['target_ids'] = prompt_df['response'].apply(lambda x: tokenizer.encode(x, return_tensors='pt').squeeze(0))\n","\n","# Sample the dataset\n","test_dataset = prompt_df.sample(2000)\n","\n","def evaluate_model(model, tokenizer, test_dataset, batch_size=32):\n","    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    rouge1_scores = []\n","    rouge2_scores = []\n","    rougeL_scores = []\n","    bleu_scores = []\n","\n","    model.eval()\n","    num_examples = len(test_dataset)\n","    num_batches = (num_examples + batch_size - 1) // batch_size\n","\n","    with torch.no_grad():\n","        for i in tqdm(range(num_batches)):\n","            start_idx = i * batch_size\n","            end_idx = min((i + 1) * batch_size, num_examples)\n","            batch = test_dataset[start_idx:end_idx]\n","\n","            input_texts = []\n","            reference_texts = []\n","\n","            # Access the input_ids and target_ids from the batch\n","            for input_ids, target_ids in zip(batch['input_ids'], batch['target_ids']):\n","                input_texts.append(tokenizer.decode(input_ids, skip_special_tokens=False))\n","                reference_texts.append(tokenizer.decode(target_ids, skip_special_tokens=False))\n","\n","            # Generate predictions\n","            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n","            outputs = model.generate(\n","                inputs[\"input_ids\"].to(model.device),\n","                max_length=100,\n","                num_return_sequences=1,\n","                temperature=0.7,\n","                no_repeat_ngram_size=2,\n","                pad_token_id=tokenizer.pad_token_id\n","            )\n","\n","            predicted_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","\n","            # Calculate scores for each example in batch\n","            for pred, ref in zip(predicted_texts, reference_texts):\n","                # ROUGE scores\n","                rouge_scores = rouge.score(pred, ref)\n","                rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n","                rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n","                rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n","\n","                # BLEU score\n","                pred_tokens = pred.split()\n","                ref_tokens = ref.split()\n","                if len(pred_tokens) > 0 and len(ref_tokens) > 0:\n","                    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)\n","                    bleu_scores.append(bleu)\n","\n","    results = {\n","        'rouge1': np.mean(rouge1_scores),\n","        'rouge2': np.mean(rouge2_scores),\n","        'rougeL': np.mean(rougeL_scores),\n","        'bleu': np.mean(bleu_scores)\n","    }\n","\n","    results.update({\n","        'rouge1_std': np.std(rouge1_scores),\n","        'rouge2_std': np.std(rouge2_scores),\n","        'rougeL_std': np.std(rougeL_scores),\n","        'bleu_std': np.std(bleu_scores)\n","    })\n","\n","    return results\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T20:13:44.560798Z","iopub.status.busy":"2024-10-18T20:13:44.559717Z","iopub.status.idle":"2024-10-18T20:25:03.986776Z","shell.execute_reply":"2024-10-18T20:25:03.985833Z","shell.execute_reply.started":"2024-10-18T20:13:44.560742Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/63 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","  2%|▏         | 1/63 [00:11<11:24, 11.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","  3%|▎         | 2/63 [00:21<11:01, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","  5%|▍         | 3/63 [00:32<10:45, 10.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","  6%|▋         | 4/63 [00:43<10:32, 10.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","  8%|▊         | 5/63 [00:54<10:27, 10.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 10%|▉         | 6/63 [01:05<10:23, 10.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 11%|█         | 7/63 [01:16<10:13, 10.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 13%|█▎        | 8/63 [01:27<10:08, 11.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 14%|█▍        | 9/63 [01:37<09:46, 10.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 16%|█▌        | 10/63 [01:48<09:36, 10.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 17%|█▋        | 11/63 [02:00<09:33, 11.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 19%|█▉        | 12/63 [02:10<09:17, 10.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 21%|██        | 13/63 [02:22<09:08, 10.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 22%|██▏       | 14/63 [02:33<08:58, 11.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 24%|██▍       | 15/63 [02:43<08:43, 10.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 25%|██▌       | 16/63 [02:54<08:35, 10.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 27%|██▋       | 17/63 [03:05<08:13, 10.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 29%|██▊       | 18/63 [03:15<08:01, 10.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 30%|███       | 19/63 [03:26<07:56, 10.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 32%|███▏      | 20/63 [03:37<07:43, 10.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 33%|███▎      | 21/63 [03:48<07:30, 10.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 35%|███▍      | 22/63 [03:59<07:24, 10.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 37%|███▋      | 23/63 [04:09<07:10, 10.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 38%|███▊      | 24/63 [04:20<06:58, 10.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 40%|███▉      | 25/63 [04:31<06:50, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 41%|████▏     | 26/63 [04:42<06:39, 10.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 43%|████▎     | 27/63 [04:52<06:29, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 44%|████▍     | 28/63 [05:04<06:23, 10.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 46%|████▌     | 29/63 [05:14<06:09, 10.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 48%|████▊     | 30/63 [05:25<05:56, 10.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 49%|████▉     | 31/63 [05:36<05:47, 10.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 51%|█████     | 32/63 [05:47<05:35, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 52%|█████▏    | 33/63 [05:58<05:24, 10.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 54%|█████▍    | 34/63 [06:09<05:16, 10.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 56%|█████▌    | 35/63 [06:19<05:03, 10.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 57%|█████▋    | 36/63 [06:30<04:52, 10.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 59%|█████▊    | 37/63 [06:41<04:43, 10.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 60%|██████    | 38/63 [06:52<04:31, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 62%|██████▏   | 39/63 [07:03<04:18, 10.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 63%|██████▎   | 40/63 [07:14<04:11, 10.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 65%|██████▌   | 41/63 [07:24<03:57, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 67%|██████▋   | 42/63 [07:35<03:46, 10.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 68%|██████▊   | 43/63 [07:46<03:38, 10.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 70%|██████▉   | 44/63 [07:57<03:26, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 71%|███████▏  | 45/63 [08:08<03:14, 10.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 73%|███████▎  | 46/63 [08:19<03:03, 10.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 75%|███████▍  | 47/63 [08:29<02:51, 10.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 76%|███████▌  | 48/63 [08:40<02:41, 10.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 78%|███████▊  | 49/63 [08:51<02:31, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 79%|███████▉  | 50/63 [09:02<02:20, 10.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 81%|████████  | 51/63 [09:12<02:08, 10.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 83%|████████▎ | 52/63 [09:23<01:58, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 84%|████████▍ | 53/63 [09:34<01:47, 10.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 86%|████████▌ | 54/63 [09:44<01:35, 10.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 87%|████████▋ | 55/63 [09:55<01:26, 10.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 89%|████████▉ | 56/63 [10:06<01:14, 10.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 90%|█████████ | 57/63 [10:16<01:04, 10.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 92%|█████████▏| 58/63 [10:28<00:54, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 94%|█████████▎| 59/63 [10:39<00:43, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 95%|█████████▌| 60/63 [10:49<00:32, 10.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 97%|█████████▋| 61/63 [11:00<00:21, 10.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"," 98%|█████████▊| 62/63 [11:11<00:10, 10.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","100%|██████████| 63/63 [11:19<00:00, 10.78s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Evaluation Results:\n","==================================================\n","ROUGE-1  : 0.4458 ± 0.1324\n","ROUGE-2  : 0.2005 ± 0.1188\n","ROUGE-L  : 0.2959 ± 0.1089\n","BLEU     : 0.0950 ± 0.0912\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def print_evaluation_results(results):\n","    \"\"\"Pretty print the evaluation results\"\"\"\n","    print(\"\\nEvaluation Results:\")\n","    print(\"=\" * 50)\n","    metrics = [\n","        ('ROUGE-1', 'rouge1'),\n","        ('ROUGE-2', 'rouge2'),\n","        ('ROUGE-L', 'rougeL'),\n","        ('BLEU', 'bleu')\n","    ]\n","\n","    for metric_name, metric_key in metrics:\n","        mean = results[metric_key]\n","        std = results[f'{metric_key}_std']\n","        print(f\"{metric_name:8} : {mean:.4f} ± {std:.4f}\")\n","\n","# Example usage\n","\n","results = evaluate_model(model, tokenizer, test_dataset)\n","print_evaluation_results(results)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":55151,"sourceId":2669146,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
